{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOHRLs9LRPI",
        "colab_type": "text"
      },
      "source": [
        "# A3: Three practical tools: TensorBoard, LIME, Keras Tuner\n",
        "\n",
        "## About\n",
        "\n",
        "In this assignment, you will gain hands-on experience with three practical tools. This assignment also includes several questions which require written responses (length: about a paragraph each). Please write your answers in this notebook in the text cells provided.\n",
        "\n",
        "Starter code showing how to use TensorBoard.dev is provided at the end of this notebook.\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "To submit this assignment, please save your notebook (with output), and upload it to CourseWorks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MJG_gaKlF_2",
        "colab_type": "text"
      },
      "source": [
        "# Section 1: Run experiments and visualize your results using TensorBoard\n",
        "\n",
        "Deep Learning is a new and developing field. Frequently, papers are published introducing new activation functions, weight initialization strategies,  optimizers, and layers. These papers often include results on large dataets (ImageNet), but whether the techniques they introduce will generalize to your datasets (which are often much smaller) remains unclear. The best way to determine that is by running an experiment.\n",
        "\n",
        "## Swish\n",
        "\n",
        "The current \"default\" activation function is ReLU (informally, our experience is that while it may not be optimal for all datasets, it is usually a reasonable choice). In the paper [Searching for Activation Functions](https://arxiv.org/abs/1710.05941), the authors propose a new activation function called \"Swish\", which they believe may become the default in the future. \n",
        "\n",
        "## Written Answers\n",
        "Please answer the following questions with a clear and concice one paragraph response each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIQUbXXYjUeq",
        "colab_type": "text"
      },
      "source": [
        "## 1a) Why is ReLU activation prefered over Sigmoid?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYw66Hl-jVxu",
        "colab_type": "text"
      },
      "source": [
        " ReLU is generally the prefered activation function over Sigmoid for a number of reasons. One being that the ReLU function is less computaionally expensive as the sigmoid function. An additional advantage is that the ReLU function does not have the issue of the vanishing gradient seen with the sigmoid function. For the sigmoid function when the absolute value of the input increase the derivative of the function decreases. This phenomenon can lead to stagnation issues while attempting to compute gradient decent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzT50fTXjW7h",
        "colab_type": "text"
      },
      "source": [
        "## 1b) What is the Vanishing Gradient problem? Specifically, when and why does it occur in a MLP?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DtQsj2NjhES",
        "colab_type": "text"
      },
      "source": [
        "As described above when training a MLP using backpropogation the weights of the network are updated based on the derivative of the error function which is calculated based upon the activation function of each neuron. The issue arrises when the partial derivative of the error function becomes very small due to the nature of the activation function. This causes the optimization process to become  stagnant. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNCVlyV8jYYu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Programming questions\n",
        "Please answer the following questions by writing code in this notebook (in the cells provided below) and saving your output.\n",
        "\n",
        "## 1c) Implement the Swish activation function using TensorFlow 2.1\n",
        "\n",
        "Note: Swish is [available](https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish) in tf-nightly. Do not use the built-in implemention. Instead, you should write a custom activation function (as if it was not yet available in the codebase). You do *not* need to extend TensorFlow to make your Swish implementation available everywhere (just write code to make it work in this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ-tS3FIjZ-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# Feel free to add additional code cells if helpful\n",
        "def swish(x):  \n",
        "  return tf.keras.activations.relu(x)*tf.keras.activations.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCTCoMP1ja-H",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1d) Run an experiment to see whether Swish is helpful on a small dataset\n",
        "\n",
        "- Choose a small datasets (e.g., CIFAR-10). \n",
        "- Train two NNs: One using ReLu activation, and one using your Swish implementation. \n",
        "- Visualize the learning curves for training & validation loss and accuracy using TensorBoard.dev (you can find starter code for TensorBoard.dev at the bottom of this notebook). \n",
        "- Fix the TensorFlow random seed before running these experiments, so your results are reproducibile (you can find a notebook on CourseWorks that demonstrates how to fix the seed).\n",
        "\n",
        "Note: It may be difficult to see whether Swish is helpful with a small experiment like this, and it's likely that your learning curves will be noisy. You do not need to run a perfect experiment (the goal is for you to gain experience implementing a custom activation function, and to learn how to use TensorBoard).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tBYp8UXjcbX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "279ce08b-3692-4ceb-f3b1-26876752707c"
      },
      "source": [
        "# TODO: your code here\n",
        "# You may add additional code cells if helpful\n",
        "# You can find example code for TensorBoard.dev at the bottom of this notebook\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puUpsb9flgNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime \n",
        "import os\n",
        "\n",
        "random.seed(42) # not necessary, but may be helpful if you have other code\n",
        "np.random.seed(42) # not necessary, but may be helpful if you have other code\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJzhkfAlL2PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.astype('float32') / 255, y_train)).shuffle(60000).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_test.astype('float32') / 255, y_test)).batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elQfIOsLwx0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmAwK0zzis9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "20cdf926-88d5-4682-a544-4c0aae7e0712"
      },
      "source": [
        "## RELU\n",
        "class MyDNN(Model):\n",
        "  def __init__(self):\n",
        "    super(MyDNN, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(32, 32, 3))\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(32, activation='relu')\n",
        "    self.d3 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    # print(x)\n",
        "    return self.d3(x)\n",
        "\n",
        "model = MyDNN()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Relu - train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Relu - test\"))\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20200321-154153\n",
            "Epoch 1, Loss: 0.007741194684058428, Accuracy: 99.75167083740234, Test Loss: 0.11346880346536636, Test Accuracy: 98.02999877929688\n",
            "Epoch 2, Loss: 0.008515648543834686, Accuracy: 99.69166564941406, Test Loss: 0.14172124862670898, Test Accuracy: 97.69999694824219\n",
            "Epoch 3, Loss: 0.007386624813079834, Accuracy: 99.75666046142578, Test Loss: 0.14231133460998535, Test Accuracy: 97.72999572753906\n",
            "Epoch 4, Loss: 0.008624139241874218, Accuracy: 99.71500396728516, Test Loss: 0.134721577167511, Test Accuracy: 97.82999420166016\n",
            "Epoch 5, Loss: 0.008154097944498062, Accuracy: 99.75666046142578, Test Loss: 0.128207728266716, Test Accuracy: 97.7699966430664\n",
            "Epoch 6, Loss: 0.007297883741557598, Accuracy: 99.7550048828125, Test Loss: 0.12240876257419586, Test Accuracy: 97.93999481201172\n",
            "Epoch 7, Loss: 0.01007083710283041, Accuracy: 99.68666076660156, Test Loss: 0.14332430064678192, Test Accuracy: 97.81999969482422\n",
            "Epoch 8, Loss: 0.006173454690724611, Accuracy: 99.80500030517578, Test Loss: 0.1481049805879593, Test Accuracy: 97.68000030517578\n",
            "Epoch 9, Loss: 0.007610759697854519, Accuracy: 99.7699966430664, Test Loss: 0.14671166241168976, Test Accuracy: 97.80999755859375\n",
            "Epoch 10, Loss: 0.009663171134889126, Accuracy: 99.7266616821289, Test Loss: 0.13809552788734436, Test Accuracy: 97.87999725341797\n",
            "Epoch 11, Loss: 0.005257864948362112, Accuracy: 99.85166931152344, Test Loss: 0.15348616242408752, Test Accuracy: 97.5999984741211\n",
            "Epoch 12, Loss: 0.007949617691338062, Accuracy: 99.7249984741211, Test Loss: 0.14883066713809967, Test Accuracy: 97.69999694824219\n",
            "Epoch 13, Loss: 0.006884769070893526, Accuracy: 99.7733383178711, Test Loss: 0.1583305150270462, Test Accuracy: 97.75\n",
            "Epoch 14, Loss: 0.005964437499642372, Accuracy: 99.83333587646484, Test Loss: 0.1539859175682068, Test Accuracy: 97.77999877929688\n",
            "Epoch 15, Loss: 0.006049501243978739, Accuracy: 99.8116683959961, Test Loss: 0.17889702320098877, Test Accuracy: 97.3699951171875\n",
            "Epoch 16, Loss: 0.008157284930348396, Accuracy: 99.75, Test Loss: 0.17278122901916504, Test Accuracy: 97.7199935913086\n",
            "Epoch 17, Loss: 0.004864348564296961, Accuracy: 99.83999633789062, Test Loss: 0.16683414578437805, Test Accuracy: 97.8499984741211\n",
            "Epoch 18, Loss: 0.007403404451906681, Accuracy: 99.7683334350586, Test Loss: 0.1886887401342392, Test Accuracy: 97.66999816894531\n",
            "Epoch 19, Loss: 0.007839247584342957, Accuracy: 99.7933349609375, Test Loss: 0.16627509891986847, Test Accuracy: 97.72999572753906\n",
            "Epoch 20, Loss: 0.00502903014421463, Accuracy: 99.8550033569336, Test Loss: 0.16669099032878876, Test Accuracy: 97.7199935913086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQjRiz0Hy17L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Swish\n",
        "class MyDNN(Model):\n",
        "  def __init__(self):\n",
        "    super(MyDNN, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(32, 32, 3))\n",
        "    self.d1 = Dense(128)\n",
        "    self.d2 = Dense(32)\n",
        "    self.d3 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = swish(x)\n",
        "    x = self.d2(x)\n",
        "    x = swish(x)\n",
        "    # print(x)\n",
        "    return self.d3(x)\n",
        "\n",
        "model = MyDNN()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Swish - train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Swish - test\"))\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFE_Ttv0plJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "32a849e3-281d-47c5-d985-5c4d32795286"
      },
      "source": [
        "!tensorboard dev upload --logdir \"$log_dir\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/default.py\", line 34, in <module>\n",
            "    import pkg_resources\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3254, in <module>\n",
            "    @_call_aside\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3238, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 3267, in _initialize_master_working_set\n",
            "    working_set = WorkingSet._build_master()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 575, in _build_master\n",
            "    ws = cls()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 568, in __init__\n",
            "    self.add_entry(entry)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 624, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2067, in find_on_path\n",
            "    for dist in factory(fullpath):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\", line 2128, in distributions_from_metadata\n",
            "    if os.path.isdir(path):\n",
            "  File \"/usr/lib/python3.6/genericpath.py\", line 42, in isdir\n",
            "    st = os.stat(s)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 5, in <module>\n",
            "    from tensorboard.main import run_main\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 40, in <module>\n",
            "    from tensorboard import default\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 319, in __exit__\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gp3ZsXRjdSb",
        "colab_type": "text"
      },
      "source": [
        "## 1e) Include a brief written answer to the following questions:\n",
        "- Did Swish help your NNs reach a higher validation accuracy? \n",
        "- Did it reduce the time (in terms of training epochs) needed to reach a certain accuracy?\n",
        "- Please include the URLs showing the results of your experiments in TensorBoard.dev to justify your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCxpZ5PkwP6",
        "colab_type": "text"
      },
      "source": [
        "### TODO: your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3h8P2lylHFF",
        "colab_type": "text"
      },
      "source": [
        "# Section 2: Use LIME to explain an image classifier\n",
        "\n",
        "Explaining and interpreting models is a new and increasingly important area of Deep Learning. In this section, you will gain experience using a recent (and relatively simple) technique called LIME.\n",
        "\n",
        "## Written answers\n",
        "\n",
        "## 2a) Read the paper [ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases](https://arxiv.org/abs/1711.11443) then answer the following two questions:\n",
        "- Why would a model misclassify a professional sports player based on the color of their skin?\n",
        "- What changes would you make to the training set to correct this behavior?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-uVNVbvjE_Y",
        "colab_type": "text"
      },
      "source": [
        "### TODO: your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr1FiS20k57c",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Programming questions\n",
        "\n",
        "## 2b) Read the paper [\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) then practice using LIME by adding code below to do the following:\n",
        "- Install LIME\n",
        "- Download an image from the web (using `!wget` or a similar utility).\n",
        "- Classify your image using Inception-V3 (or another famous architecture, using weights pretrained on ImageNet).\n",
        "- Display the top three predicted classes (e.g., baseball player) and confidence scores.\n",
        "- Use LIME to provide evidence for and against each of the top three predictions (e.g., display the regions of an image that LIME found to correlate most strongly with and against the predicted class).\n",
        "\n",
        "Save your output inside this notebook and include it with your submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZVX3z9ak9G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# You may add additional text cells if helpful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXbHV3Gdk7i8",
        "colab_type": "text"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "If you like to learn about another promising approach to explaining NNs, you can read the paper [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6ZYCN2eoC7",
        "colab_type": "text"
      },
      "source": [
        "# Section 3: Use Keras Tuner to optimize a small model\n",
        "\n",
        "\n",
        "## Written answers\n",
        "\n",
        "## 3a) When and why might Grid Search be less effective than Random Search when searching for useful hyperparameters for a Deep Learning model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKQXRhCblS-J",
        "colab_type": "text"
      },
      "source": [
        "### TODO: your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2km45q83lVcV",
        "colab_type": "text"
      },
      "source": [
        "## Programming questions\n",
        "\n",
        "**3b) Add one or more code cells below in which you complete the following:**\n",
        "- Install Keras Tuner\n",
        "- Write a CNN to classify images from CIFAR-10 \n",
        "- Use Keras Tuner to search for at least three optimal hyperpameters for your model (eg, number of layers, number of filters per layer, dropout rate, etc)\n",
        "\n",
        "Save your output in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cth4UWZelU9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# You may add additional code cells if helpful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBVypFcola6l",
        "colab_type": "text"
      },
      "source": [
        "## 3c) In the text cell below, brielfy answer the following questions:\n",
        "- What were the optimal hyperparmeters you found?\n",
        "- Do they make sense (e.g., are they similar to parameters you might have picked yourself?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYYvxfpOldSt",
        "colab_type": "text"
      },
      "source": [
        "### TODO: your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjU1L133f4sp",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSc6v5ws8wKS",
        "colab_type": "text"
      },
      "source": [
        "## Starter code for TensorBoard\n",
        "The following code shows how to use [TensorBoard](https://www.tensorflow.org/tensorboard) to display the results from an experiment comparing two learning curves. Please note, there are three ways to use TensorBoard. \n",
        "- You may install TensorBoard locally on your laptop\n",
        "- You can run TensorBoard inside Colab (currently buggy)\n",
        "- You can use TensorBoard.dev (this is the approach you should use for this assignment). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE5lIlzkEUAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "599JJ57yl1rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpe_UINKKLDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehUDr72LLAJY"
      },
      "source": [
        "**Caution**. The following cell will delete any existing TensorBoard logs. If you're running this on your local machine, please be careful executing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScjIcVAJKkmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ6aWI_NLm6C",
        "colab_type": "text"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByT9RfkIK4kD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "22ffcb3a-d584-4651-a8b0-7ca63c31abbd"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2_huYeq80tv",
        "colab_type": "text"
      },
      "source": [
        "## First style\n",
        "The following code shows how to use TensorBoard with ```model.fit```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHBo5cgTLuis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEDGbBRMI-_",
        "colab_type": "text"
      },
      "source": [
        "Create a logs directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggu_lZRnL6Uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime \n",
        "import os\n",
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_icHggivMBXe",
        "colab_type": "text"
      },
      "source": [
        "### Run an experiment\n",
        "The name of the experiment is given by the path of the logs directory (here, \"exp1\"). You'll want to use something more descriptive in your work (e.g., \"swish-cifar-10\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyVj4vmLM0Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.0, nesterov=False) \n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"exp1\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6-cL05DN9fR",
        "colab_type": "text"
      },
      "source": [
        "### Run a second experiment\n",
        "Let's train another model, this time saving results to \"exp2\". Later, we'll be able to compare the learning curves of these experiments in TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XVPmrpeN_u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"exp2\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_pSh18ZOENe",
        "colab_type": "text"
      },
      "source": [
        "### Upload the logs to TensorBoard.dev, and compare the results\n",
        "TensorBoard.dev is a hosted version of TensorBoard (see http://tensorboard.dev/ for details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NwwW6OEUAIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard dev upload --logdir \"$log_dir\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPLDEq8MSQYH",
        "colab_type": "text"
      },
      "source": [
        "## Second style\n",
        "Showing how to use TensorBoard with a Subclassed model and a GradientTape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vV0Hz2RV1C5",
        "colab_type": "text"
      },
      "source": [
        "Prepre the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX0umywbPeW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_ds = train_ds.shuffle(60000).batch(32)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZgdwTLLwq1D",
        "colab_type": "text"
      },
      "source": [
        "Define a simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRaR1rtpUgDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    return self.d1(x)\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMFRp7CCTxmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aIkldyT1S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZzldV1BV3Rs",
        "colab_type": "text"
      },
      "source": [
        "Training and testing routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDHIccXUP_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEUmM7G6V7ae",
        "colab_type": "text"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGrEExdWV84_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fde13255-6f4a-4bce-abf0-274b72f61570"
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"test\"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20200321-152917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v-gMTCiwuBO",
        "colab_type": "text"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rxTf4CyVKxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "cc83bb27-8485-4eb9-cec9-854046c25a89"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4738991856575012, Accuracy: 87.65833282470703, Test Loss: 0.30589091777801514, Test Accuracy: 91.52999877929688\n",
            "Epoch 2, Loss: 0.30400350689888, Accuracy: 91.5816650390625, Test Loss: 0.28006136417388916, Test Accuracy: 92.25\n",
            "Epoch 3, Loss: 0.2830759882926941, Accuracy: 92.10166931152344, Test Loss: 0.27733173966407776, Test Accuracy: 92.29999542236328\n",
            "Epoch 4, Loss: 0.2731097638607025, Accuracy: 92.36000061035156, Test Loss: 0.27061912417411804, Test Accuracy: 92.44999694824219\n",
            "Epoch 5, Loss: 0.2666522264480591, Accuracy: 92.5633316040039, Test Loss: 0.2657223045825958, Test Accuracy: 92.69999694824219\n",
            "Epoch 6, Loss: 0.26179182529449463, Accuracy: 92.6933364868164, Test Loss: 0.26932454109191895, Test Accuracy: 92.45999908447266\n",
            "Epoch 7, Loss: 0.2584364116191864, Accuracy: 92.84666442871094, Test Loss: 0.26669907569885254, Test Accuracy: 92.69000244140625\n",
            "Epoch 8, Loss: 0.2554880678653717, Accuracy: 92.86833190917969, Test Loss: 0.2611234486103058, Test Accuracy: 92.83999633789062\n",
            "Epoch 9, Loss: 0.25281357765197754, Accuracy: 93.00666809082031, Test Loss: 0.27040645480155945, Test Accuracy: 92.38999938964844\n",
            "Epoch 10, Loss: 0.25119897723197937, Accuracy: 93.038330078125, Test Loss: 0.26565492153167725, Test Accuracy: 92.58000183105469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9-eBZGIo5NZ",
        "colab_type": "text"
      },
      "source": [
        "### Upload the logs to TensorBoard.dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrXOlQa5VWnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard dev upload --logdir \"$log_dir\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}