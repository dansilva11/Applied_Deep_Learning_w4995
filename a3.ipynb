{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOHRLs9LRPI",
        "colab_type": "text"
      },
      "source": [
        "# A3: Three practical tools: TensorBoard, LIME, Keras Tuner\n",
        "\n",
        "## About\n",
        "\n",
        "In this assignment, you will gain hands-on experience with three practical tools. This assignment also includes several questions which require written responses (length: about a paragraph each). Please write your answers in this notebook in the text cells provided.\n",
        "\n",
        "Starter code showing how to use TensorBoard.dev is provided at the end of this notebook.\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "To submit this assignment, please save your notebook (with output), and upload it to CourseWorks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MJG_gaKlF_2",
        "colab_type": "text"
      },
      "source": [
        "# Section 1: Run experiments and visualize your results using TensorBoard\n",
        "\n",
        "Deep Learning is a new and developing field. Frequently, papers are published introducing new activation functions, weight initialization strategies,  optimizers, and layers. These papers often include results on large dataets (ImageNet), but whether the techniques they introduce will generalize to your datasets (which are often much smaller) remains unclear. The best way to determine that is by running an experiment.\n",
        "\n",
        "## Swish\n",
        "\n",
        "The current \"default\" activation function is ReLU (informally, our experience is that while it may not be optimal for all datasets, it is usually a reasonable choice). In the paper [Searching for Activation Functions](https://arxiv.org/abs/1710.05941), the authors propose a new activation function called \"Swish\", which they believe may become the default in the future. \n",
        "\n",
        "## Written Answers\n",
        "Please answer the following questions with a clear and concice one paragraph response each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIQUbXXYjUeq",
        "colab_type": "text"
      },
      "source": [
        "## 1a) Why is ReLU activation prefered over Sigmoid?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYw66Hl-jVxu",
        "colab_type": "text"
      },
      "source": [
        " ReLU is generally the prefered activation function over Sigmoid for a number of reasons. One being that the ReLU function is less computaionally expensive as the sigmoid function. An additional advantage is that the ReLU function does not have the issue of the vanishing gradient seen with the sigmoid function. For the sigmoid function when the absolute value of the input increase the derivative of the function decreases. This phenomenon can lead to stagnation issues while attempting to compute gradient decent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzT50fTXjW7h",
        "colab_type": "text"
      },
      "source": [
        "## 1b) What is the Vanishing Gradient problem? Specifically, when and why does it occur in a MLP?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DtQsj2NjhES",
        "colab_type": "text"
      },
      "source": [
        "As described above when training a MLP using backpropogation the weights of the network are updated based on the derivative of the error function which is calculated based upon the activation function of each neuron. The issue arrises when the partial derivative of the error function becomes very small due to the nature of the activation function. This causes the optimization process to become  stagnant. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNCVlyV8jYYu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Programming questions\n",
        "Please answer the following questions by writing code in this notebook (in the cells provided below) and saving your output.\n",
        "\n",
        "## 1c) Implement the Swish activation function using TensorFlow 2.1\n",
        "\n",
        "Note: Swish is [available](https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish) in tf-nightly. Do not use the built-in implemention. Instead, you should write a custom activation function (as if it was not yet available in the codebase). You do *not* need to extend TensorFlow to make your Swish implementation available everywhere (just write code to make it work in this notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ-tS3FIjZ-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# Feel free to add additional code cells if helpful\n",
        "\n",
        "def swish(x):  \n",
        "  return x*tf.keras.activations.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCTCoMP1ja-H",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1d) Run an experiment to see whether Swish is helpful on a small dataset\n",
        "\n",
        "- Choose a small datasets (e.g., CIFAR-10). \n",
        "- Train two NNs: One using ReLu activation, and one using your Swish implementation. \n",
        "- Visualize the learning curves for training & validation loss and accuracy using TensorBoard.dev (you can find starter code for TensorBoard.dev at the bottom of this notebook). \n",
        "- Fix the TensorFlow random seed before running these experiments, so your results are reproducibile (you can find a notebook on CourseWorks that demonstrates how to fix the seed).\n",
        "\n",
        "Note: It may be difficult to see whether Swish is helpful with a small experiment like this, and it's likely that your learning curves will be noisy. You do not need to run a perfect experiment (the goal is for you to gain experience implementing a custom activation function, and to learn how to use TensorBoard).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tBYp8UXjcbX",
        "colab_type": "code",
        "outputId": "dc84851d-1345-4a41-e48d-4daf8537872d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# TODO: your code here\n",
        "# You may add additional code cells if helpful\n",
        "# You can find example code for TensorBoard.dev at the bottom of this notebook\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puUpsb9flgNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime \n",
        "import os\n",
        "\n",
        "random.seed(42) # not necessary, but may be helpful if you have other code\n",
        "np.random.seed(42) # not necessary, but may be helpful if you have other code\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJzhkfAlL2PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.astype('float32') / 255, y_train)).shuffle(60000).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_test.astype('float32') / 255, y_test)).batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elQfIOsLwx0h",
        "colab_type": "code",
        "outputId": "c5315e2f-c792-4c9c-e5fa-5e33fd2115c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20200322-152636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmAwK0zzis9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe181af4-5430-4dd5-9dc1-44b8d0b06994"
      },
      "source": [
        "## RELU\n",
        "class MyDNN(Model):\n",
        "  def __init__(self):\n",
        "    super(MyDNN, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(32, 32, 32, 3))\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(64, activation='relu')\n",
        "    self.d3 = Dense(32, activation='relu')\n",
        "    self.d4 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.d3(x)\n",
        "    # print(x)\n",
        "    return self.d4(x)\n",
        "\n",
        "model = MyDNN()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Relu - train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Relu - test\"))\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.8908886909484863, Accuracy: 31.536001205444336, Test Loss: 1.7378672361373901, Test Accuracy: 36.959999084472656\n",
            "Epoch 2, Loss: 1.7204782962799072, Accuracy: 37.970001220703125, Test Loss: 1.6853394508361816, Test Accuracy: 39.82999801635742\n",
            "Epoch 3, Loss: 1.648645043373108, Accuracy: 40.444000244140625, Test Loss: 1.6040765047073364, Test Accuracy: 42.13999938964844\n",
            "Epoch 4, Loss: 1.5921355485916138, Accuracy: 42.75400161743164, Test Loss: 1.5848504304885864, Test Accuracy: 42.959999084472656\n",
            "Epoch 5, Loss: 1.5578539371490479, Accuracy: 43.93199920654297, Test Loss: 1.5999211072921753, Test Accuracy: 43.0099983215332\n",
            "Epoch 6, Loss: 1.530818223953247, Accuracy: 45.209999084472656, Test Loss: 1.5440140962600708, Test Accuracy: 44.459999084472656\n",
            "Epoch 7, Loss: 1.502284288406372, Accuracy: 46.09000015258789, Test Loss: 1.5486111640930176, Test Accuracy: 44.130001068115234\n",
            "Epoch 8, Loss: 1.4842950105667114, Accuracy: 46.79600143432617, Test Loss: 1.5453975200653076, Test Accuracy: 44.70000076293945\n",
            "Epoch 9, Loss: 1.4669287204742432, Accuracy: 47.391998291015625, Test Loss: 1.464200496673584, Test Accuracy: 47.63999938964844\n",
            "Epoch 10, Loss: 1.4488078355789185, Accuracy: 48.07400131225586, Test Loss: 1.4868006706237793, Test Accuracy: 46.439998626708984\n",
            "Epoch 11, Loss: 1.434174656867981, Accuracy: 48.60200119018555, Test Loss: 1.4577809572219849, Test Accuracy: 48.220001220703125\n",
            "Epoch 12, Loss: 1.4220068454742432, Accuracy: 48.974002838134766, Test Loss: 1.5010625123977661, Test Accuracy: 46.59000015258789\n",
            "Epoch 13, Loss: 1.4107526540756226, Accuracy: 49.59600067138672, Test Loss: 1.532464623451233, Test Accuracy: 45.540000915527344\n",
            "Epoch 14, Loss: 1.40041184425354, Accuracy: 49.63199996948242, Test Loss: 1.5146063566207886, Test Accuracy: 46.5\n",
            "Epoch 15, Loss: 1.3888194561004639, Accuracy: 50.0620002746582, Test Loss: 1.4765276908874512, Test Accuracy: 47.40999984741211\n",
            "Epoch 16, Loss: 1.3785351514816284, Accuracy: 50.63999938964844, Test Loss: 1.4430711269378662, Test Accuracy: 48.36000061035156\n",
            "Epoch 17, Loss: 1.3674418926239014, Accuracy: 51.040000915527344, Test Loss: 1.4584728479385376, Test Accuracy: 47.86000061035156\n",
            "Epoch 18, Loss: 1.3644613027572632, Accuracy: 51.12599563598633, Test Loss: 1.448852777481079, Test Accuracy: 48.48999786376953\n",
            "Epoch 19, Loss: 1.3531049489974976, Accuracy: 51.492000579833984, Test Loss: 1.4992948770523071, Test Accuracy: 46.7400016784668\n",
            "Epoch 20, Loss: 1.3456861972808838, Accuracy: 51.57600021362305, Test Loss: 1.4302194118499756, Test Accuracy: 49.29999923706055\n",
            "Epoch 21, Loss: 1.3362431526184082, Accuracy: 52.013999938964844, Test Loss: 1.4530224800109863, Test Accuracy: 47.71000289916992\n",
            "Epoch 22, Loss: 1.3323014974594116, Accuracy: 52.21800231933594, Test Loss: 1.4917360544204712, Test Accuracy: 47.130001068115234\n",
            "Epoch 23, Loss: 1.323535680770874, Accuracy: 52.513999938964844, Test Loss: 1.417409896850586, Test Accuracy: 49.720001220703125\n",
            "Epoch 24, Loss: 1.3189880847930908, Accuracy: 52.55400085449219, Test Loss: 1.4629299640655518, Test Accuracy: 48.12000274658203\n",
            "Epoch 25, Loss: 1.3082021474838257, Accuracy: 53.09600067138672, Test Loss: 1.4724260568618774, Test Accuracy: 48.34000015258789\n",
            "Epoch 26, Loss: 1.303568959236145, Accuracy: 53.15399932861328, Test Loss: 1.4622212648391724, Test Accuracy: 48.349998474121094\n",
            "Epoch 27, Loss: 1.2973816394805908, Accuracy: 53.555999755859375, Test Loss: 1.4737731218338013, Test Accuracy: 47.42000198364258\n",
            "Epoch 28, Loss: 1.2907166481018066, Accuracy: 53.71000289916992, Test Loss: 1.4372297525405884, Test Accuracy: 49.47999954223633\n",
            "Epoch 29, Loss: 1.2869974374771118, Accuracy: 53.805999755859375, Test Loss: 1.4503188133239746, Test Accuracy: 48.400001525878906\n",
            "Epoch 30, Loss: 1.2800498008728027, Accuracy: 53.8640022277832, Test Loss: 1.4345661401748657, Test Accuracy: 48.91999816894531\n",
            "Epoch 31, Loss: 1.2779326438903809, Accuracy: 54.02799987792969, Test Loss: 1.4553178548812866, Test Accuracy: 49.02000045776367\n",
            "Epoch 32, Loss: 1.2678313255310059, Accuracy: 54.667999267578125, Test Loss: 1.4322105646133423, Test Accuracy: 50.19000244140625\n",
            "Epoch 33, Loss: 1.2629311084747314, Accuracy: 54.61199951171875, Test Loss: 1.4433314800262451, Test Accuracy: 49.20000076293945\n",
            "Epoch 34, Loss: 1.2629351615905762, Accuracy: 54.84600067138672, Test Loss: 1.4244515895843506, Test Accuracy: 49.790000915527344\n",
            "Epoch 35, Loss: 1.2593215703964233, Accuracy: 54.790000915527344, Test Loss: 1.437661051750183, Test Accuracy: 49.27000045776367\n",
            "Epoch 36, Loss: 1.249372124671936, Accuracy: 55.15800094604492, Test Loss: 1.4644147157669067, Test Accuracy: 48.56999969482422\n",
            "Epoch 37, Loss: 1.2460802793502808, Accuracy: 55.25400161743164, Test Loss: 1.4195325374603271, Test Accuracy: 50.06999969482422\n",
            "Epoch 38, Loss: 1.2442759275436401, Accuracy: 55.47200012207031, Test Loss: 1.4462991952896118, Test Accuracy: 50.11000061035156\n",
            "Epoch 39, Loss: 1.237563967704773, Accuracy: 55.85799789428711, Test Loss: 1.444837212562561, Test Accuracy: 49.459999084472656\n",
            "Epoch 40, Loss: 1.2351022958755493, Accuracy: 55.64200210571289, Test Loss: 1.4748976230621338, Test Accuracy: 48.64999771118164\n",
            "Epoch 41, Loss: 1.231223702430725, Accuracy: 55.59600067138672, Test Loss: 1.4202556610107422, Test Accuracy: 50.410003662109375\n",
            "Epoch 42, Loss: 1.2305569648742676, Accuracy: 55.88399887084961, Test Loss: 1.454472541809082, Test Accuracy: 49.12999725341797\n",
            "Epoch 43, Loss: 1.2244867086410522, Accuracy: 56.13399887084961, Test Loss: 1.4511085748672485, Test Accuracy: 49.21999740600586\n",
            "Epoch 44, Loss: 1.2221183776855469, Accuracy: 55.986000061035156, Test Loss: 1.4661062955856323, Test Accuracy: 48.91999816894531\n",
            "Epoch 45, Loss: 1.2185643911361694, Accuracy: 56.11000061035156, Test Loss: 1.467929482460022, Test Accuracy: 49.380001068115234\n",
            "Epoch 46, Loss: 1.2161011695861816, Accuracy: 56.38600158691406, Test Loss: 1.447970986366272, Test Accuracy: 49.22999954223633\n",
            "Epoch 47, Loss: 1.2171416282653809, Accuracy: 56.32200241088867, Test Loss: 1.462117314338684, Test Accuracy: 49.38999938964844\n",
            "Epoch 48, Loss: 1.2100855112075806, Accuracy: 56.551998138427734, Test Loss: 1.45880126953125, Test Accuracy: 49.15999984741211\n",
            "Epoch 49, Loss: 1.2008438110351562, Accuracy: 57.12000274658203, Test Loss: 1.4569311141967773, Test Accuracy: 49.84000015258789\n",
            "Epoch 50, Loss: 1.2016385793685913, Accuracy: 56.823997497558594, Test Loss: 1.4702733755111694, Test Accuracy: 48.73999786376953\n",
            "Epoch 51, Loss: 1.1963906288146973, Accuracy: 57.03399658203125, Test Loss: 1.493146538734436, Test Accuracy: 48.13999938964844\n",
            "Epoch 52, Loss: 1.1987037658691406, Accuracy: 56.97199630737305, Test Loss: 1.4765009880065918, Test Accuracy: 48.54999923706055\n",
            "Epoch 53, Loss: 1.1966221332550049, Accuracy: 56.70600128173828, Test Loss: 1.4564461708068848, Test Accuracy: 49.57999801635742\n",
            "Epoch 54, Loss: 1.1922547817230225, Accuracy: 57.26000213623047, Test Loss: 1.4602375030517578, Test Accuracy: 49.290000915527344\n",
            "Epoch 55, Loss: 1.1885206699371338, Accuracy: 57.22800064086914, Test Loss: 1.518433928489685, Test Accuracy: 47.96000289916992\n",
            "Epoch 56, Loss: 1.1884005069732666, Accuracy: 57.454002380371094, Test Loss: 1.4853672981262207, Test Accuracy: 49.029998779296875\n",
            "Epoch 57, Loss: 1.1848084926605225, Accuracy: 57.395999908447266, Test Loss: 1.4821014404296875, Test Accuracy: 48.15999984741211\n",
            "Epoch 58, Loss: 1.1839840412139893, Accuracy: 57.67599868774414, Test Loss: 1.4441461563110352, Test Accuracy: 50.23999786376953\n",
            "Epoch 59, Loss: 1.1753344535827637, Accuracy: 57.5620002746582, Test Loss: 1.5001847743988037, Test Accuracy: 48.65999984741211\n",
            "Epoch 60, Loss: 1.1774849891662598, Accuracy: 57.667999267578125, Test Loss: 1.454301118850708, Test Accuracy: 49.349998474121094\n",
            "Epoch 61, Loss: 1.1742254495620728, Accuracy: 57.92400360107422, Test Loss: 1.4774627685546875, Test Accuracy: 49.16999816894531\n",
            "Epoch 62, Loss: 1.169474720954895, Accuracy: 58.119998931884766, Test Loss: 1.4851235151290894, Test Accuracy: 49.45000076293945\n",
            "Epoch 63, Loss: 1.1698391437530518, Accuracy: 58.0, Test Loss: 1.4702798128128052, Test Accuracy: 49.55999755859375\n",
            "Epoch 64, Loss: 1.1661479473114014, Accuracy: 58.29199981689453, Test Loss: 1.4621044397354126, Test Accuracy: 49.30999755859375\n",
            "Epoch 65, Loss: 1.161751389503479, Accuracy: 58.32999801635742, Test Loss: 1.4751081466674805, Test Accuracy: 49.11000061035156\n",
            "Epoch 66, Loss: 1.1631401777267456, Accuracy: 58.321998596191406, Test Loss: 1.4588401317596436, Test Accuracy: 49.82999801635742\n",
            "Epoch 67, Loss: 1.1609817743301392, Accuracy: 58.263999938964844, Test Loss: 1.455632209777832, Test Accuracy: 49.31999969482422\n",
            "Epoch 68, Loss: 1.1588937044143677, Accuracy: 58.28999710083008, Test Loss: 1.4929879903793335, Test Accuracy: 49.0099983215332\n",
            "Epoch 69, Loss: 1.1563470363616943, Accuracy: 58.50600051879883, Test Loss: 1.4794870615005493, Test Accuracy: 48.91999816894531\n",
            "Epoch 70, Loss: 1.1519719362258911, Accuracy: 58.6719970703125, Test Loss: 1.5099496841430664, Test Accuracy: 48.93000030517578\n",
            "Epoch 71, Loss: 1.148269772529602, Accuracy: 58.868003845214844, Test Loss: 1.476393461227417, Test Accuracy: 49.779998779296875\n",
            "Epoch 72, Loss: 1.147889256477356, Accuracy: 58.90399932861328, Test Loss: 1.51703941822052, Test Accuracy: 48.939998626708984\n",
            "Epoch 73, Loss: 1.1477782726287842, Accuracy: 58.81599807739258, Test Loss: 1.4893041849136353, Test Accuracy: 49.849998474121094\n",
            "Epoch 74, Loss: 1.1438853740692139, Accuracy: 58.81599807739258, Test Loss: 1.4878329038619995, Test Accuracy: 49.209999084472656\n",
            "Epoch 75, Loss: 1.1395074129104614, Accuracy: 58.869998931884766, Test Loss: 1.4828487634658813, Test Accuracy: 49.290000915527344\n",
            "Epoch 76, Loss: 1.1395862102508545, Accuracy: 58.99599838256836, Test Loss: 1.4857230186462402, Test Accuracy: 49.84000015258789\n",
            "Epoch 77, Loss: 1.1396652460098267, Accuracy: 59.119998931884766, Test Loss: 1.4927947521209717, Test Accuracy: 49.380001068115234\n",
            "Epoch 78, Loss: 1.1372287273406982, Accuracy: 59.35000228881836, Test Loss: 1.495962142944336, Test Accuracy: 48.8599967956543\n",
            "Epoch 79, Loss: 1.13670814037323, Accuracy: 59.24199676513672, Test Loss: 1.50408136844635, Test Accuracy: 49.099998474121094\n",
            "Epoch 80, Loss: 1.1322805881500244, Accuracy: 59.332000732421875, Test Loss: 1.4876869916915894, Test Accuracy: 49.41999816894531\n",
            "Epoch 81, Loss: 1.127445936203003, Accuracy: 59.39999771118164, Test Loss: 1.4959548711776733, Test Accuracy: 49.52000045776367\n",
            "Epoch 82, Loss: 1.125002145767212, Accuracy: 59.58599853515625, Test Loss: 1.4862946271896362, Test Accuracy: 49.7599983215332\n",
            "Epoch 83, Loss: 1.128263235092163, Accuracy: 59.53199768066406, Test Loss: 1.509910225868225, Test Accuracy: 49.029998779296875\n",
            "Epoch 84, Loss: 1.1262010335922241, Accuracy: 59.54399871826172, Test Loss: 1.4862544536590576, Test Accuracy: 49.75\n",
            "Epoch 85, Loss: 1.1250768899917603, Accuracy: 59.451995849609375, Test Loss: 1.4989351034164429, Test Accuracy: 48.959999084472656\n",
            "Epoch 86, Loss: 1.120619297027588, Accuracy: 59.802001953125, Test Loss: 1.5239484310150146, Test Accuracy: 49.13999938964844\n",
            "Epoch 87, Loss: 1.1178666353225708, Accuracy: 59.78000259399414, Test Loss: 1.538057565689087, Test Accuracy: 48.38999938964844\n",
            "Epoch 88, Loss: 1.1143100261688232, Accuracy: 59.67000198364258, Test Loss: 1.494016170501709, Test Accuracy: 49.459999084472656\n",
            "Epoch 89, Loss: 1.1169415712356567, Accuracy: 60.0099983215332, Test Loss: 1.52340829372406, Test Accuracy: 49.27000045776367\n",
            "Epoch 90, Loss: 1.1134226322174072, Accuracy: 59.87200164794922, Test Loss: 1.4981410503387451, Test Accuracy: 49.59000015258789\n",
            "Epoch 91, Loss: 1.1123417615890503, Accuracy: 60.070003509521484, Test Loss: 1.5103895664215088, Test Accuracy: 49.12999725341797\n",
            "Epoch 92, Loss: 1.1119226217269897, Accuracy: 60.07400131225586, Test Loss: 1.5335403680801392, Test Accuracy: 48.7599983215332\n",
            "Epoch 93, Loss: 1.1066744327545166, Accuracy: 60.29199981689453, Test Loss: 1.5104159116744995, Test Accuracy: 48.79999923706055\n",
            "Epoch 94, Loss: 1.1058298349380493, Accuracy: 60.10599899291992, Test Loss: 1.5222907066345215, Test Accuracy: 48.98999786376953\n",
            "Epoch 95, Loss: 1.1047860383987427, Accuracy: 60.257999420166016, Test Loss: 1.5249438285827637, Test Accuracy: 48.79999923706055\n",
            "Epoch 96, Loss: 1.1082509756088257, Accuracy: 60.012001037597656, Test Loss: 1.5320580005645752, Test Accuracy: 48.5099983215332\n",
            "Epoch 97, Loss: 1.1013588905334473, Accuracy: 60.43199920654297, Test Loss: 1.524052619934082, Test Accuracy: 49.0099983215332\n",
            "Epoch 98, Loss: 1.0989103317260742, Accuracy: 60.3380012512207, Test Loss: 1.5364296436309814, Test Accuracy: 48.279998779296875\n",
            "Epoch 99, Loss: 1.1009610891342163, Accuracy: 60.4219970703125, Test Loss: 1.5262610912322998, Test Accuracy: 49.5\n",
            "Epoch 100, Loss: 1.0971626043319702, Accuracy: 60.60600280761719, Test Loss: 1.5569928884506226, Test Accuracy: 48.89999771118164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQjRiz0Hy17L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "994aa727-2545-47b4-99a4-bded3eae2694"
      },
      "source": [
        "## Swish\n",
        "class MyDNN(Model):\n",
        "  def __init__(self):\n",
        "    super(MyDNN, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(32, 32, 32, 3))\n",
        "    self.d1 = Dense(128)\n",
        "    self.d2 = Dense(64)\n",
        "    self.d3 = Dense(32)\n",
        "\n",
        "    self.d4 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    # print(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = swish(x)\n",
        "    x = self.d2(x)\n",
        "    x = swish(x)\n",
        "    x = self.d3(x)\n",
        "    x = swish(x)\n",
        "    # print(x)\n",
        "    return self.d4(x)\n",
        "\n",
        "model = MyDNN()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Swish - train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"Swish - test\"))\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.8574587106704712, Accuracy: 32.327999114990234, Test Loss: 1.715945839881897, Test Accuracy: 37.92000198364258\n",
            "Epoch 2, Loss: 1.680107831954956, Accuracy: 39.40999984741211, Test Loss: 1.6222938299179077, Test Accuracy: 42.46999740600586\n",
            "Epoch 3, Loss: 1.6075050830841064, Accuracy: 42.20600128173828, Test Loss: 1.5832891464233398, Test Accuracy: 43.15999984741211\n",
            "Epoch 4, Loss: 1.5575790405273438, Accuracy: 43.906002044677734, Test Loss: 1.5492335557937622, Test Accuracy: 44.8900032043457\n",
            "Epoch 5, Loss: 1.5250575542449951, Accuracy: 45.18199920654297, Test Loss: 1.5353549718856812, Test Accuracy: 45.34000015258789\n",
            "Epoch 6, Loss: 1.4996708631515503, Accuracy: 46.08399963378906, Test Loss: 1.5398486852645874, Test Accuracy: 44.630001068115234\n",
            "Epoch 7, Loss: 1.4780868291854858, Accuracy: 46.87800216674805, Test Loss: 1.5017510652542114, Test Accuracy: 47.150001525878906\n",
            "Epoch 8, Loss: 1.4600143432617188, Accuracy: 47.59599685668945, Test Loss: 1.5162891149520874, Test Accuracy: 45.96999740600586\n",
            "Epoch 9, Loss: 1.4461115598678589, Accuracy: 48.22599792480469, Test Loss: 1.4723871946334839, Test Accuracy: 47.040000915527344\n",
            "Epoch 10, Loss: 1.4255774021148682, Accuracy: 49.13600158691406, Test Loss: 1.488874077796936, Test Accuracy: 46.86000061035156\n",
            "Epoch 11, Loss: 1.4164376258850098, Accuracy: 49.54399871826172, Test Loss: 1.5062528848648071, Test Accuracy: 46.22999954223633\n",
            "Epoch 12, Loss: 1.4040770530700684, Accuracy: 49.9739990234375, Test Loss: 1.4714350700378418, Test Accuracy: 47.17000198364258\n",
            "Epoch 13, Loss: 1.392493486404419, Accuracy: 50.35200119018555, Test Loss: 1.4893854856491089, Test Accuracy: 46.83000183105469\n",
            "Epoch 14, Loss: 1.3825682401657104, Accuracy: 50.823997497558594, Test Loss: 1.5028941631317139, Test Accuracy: 46.119998931884766\n",
            "Epoch 15, Loss: 1.3743127584457397, Accuracy: 51.08199691772461, Test Loss: 1.4903310537338257, Test Accuracy: 46.93000030517578\n",
            "Epoch 16, Loss: 1.3655931949615479, Accuracy: 51.13199996948242, Test Loss: 1.4282201528549194, Test Accuracy: 48.94999694824219\n",
            "Epoch 17, Loss: 1.3560209274291992, Accuracy: 51.433998107910156, Test Loss: 1.478348970413208, Test Accuracy: 47.880001068115234\n",
            "Epoch 18, Loss: 1.3473482131958008, Accuracy: 51.81599807739258, Test Loss: 1.4659507274627686, Test Accuracy: 47.67000198364258\n",
            "Epoch 19, Loss: 1.3426814079284668, Accuracy: 51.95800018310547, Test Loss: 1.41713285446167, Test Accuracy: 49.900001525878906\n",
            "Epoch 20, Loss: 1.3343253135681152, Accuracy: 52.12600326538086, Test Loss: 1.4282702207565308, Test Accuracy: 48.69999694824219\n",
            "Epoch 21, Loss: 1.32986581325531, Accuracy: 52.49599838256836, Test Loss: 1.4366708993911743, Test Accuracy: 48.43000030517578\n",
            "Epoch 22, Loss: 1.3173235654830933, Accuracy: 52.81599807739258, Test Loss: 1.4553418159484863, Test Accuracy: 48.22999954223633\n",
            "Epoch 23, Loss: 1.3170406818389893, Accuracy: 52.93199920654297, Test Loss: 1.4248346090316772, Test Accuracy: 49.439998626708984\n",
            "Epoch 24, Loss: 1.306260585784912, Accuracy: 53.40800476074219, Test Loss: 1.4615442752838135, Test Accuracy: 48.10000228881836\n",
            "Epoch 25, Loss: 1.3022234439849854, Accuracy: 53.43600082397461, Test Loss: 1.4594792127609253, Test Accuracy: 47.95000076293945\n",
            "Epoch 26, Loss: 1.2963533401489258, Accuracy: 53.7339973449707, Test Loss: 1.4238979816436768, Test Accuracy: 49.81999969482422\n",
            "Epoch 27, Loss: 1.2944835424423218, Accuracy: 53.53000259399414, Test Loss: 1.4640371799468994, Test Accuracy: 47.970001220703125\n",
            "Epoch 28, Loss: 1.285090446472168, Accuracy: 54.354000091552734, Test Loss: 1.45968759059906, Test Accuracy: 48.290000915527344\n",
            "Epoch 29, Loss: 1.286555290222168, Accuracy: 54.00400161743164, Test Loss: 1.457465648651123, Test Accuracy: 48.10000228881836\n",
            "Epoch 30, Loss: 1.278986930847168, Accuracy: 54.104000091552734, Test Loss: 1.4326753616333008, Test Accuracy: 49.290000915527344\n",
            "Epoch 31, Loss: 1.2761847972869873, Accuracy: 54.321998596191406, Test Loss: 1.4205663204193115, Test Accuracy: 49.790000915527344\n",
            "Epoch 32, Loss: 1.272434949874878, Accuracy: 54.448001861572266, Test Loss: 1.459907054901123, Test Accuracy: 48.88999938964844\n",
            "Epoch 33, Loss: 1.2642546892166138, Accuracy: 54.86000061035156, Test Loss: 1.4633874893188477, Test Accuracy: 49.38999938964844\n",
            "Epoch 34, Loss: 1.2608388662338257, Accuracy: 55.03799819946289, Test Loss: 1.4494866132736206, Test Accuracy: 49.16999816894531\n",
            "Epoch 35, Loss: 1.2534618377685547, Accuracy: 55.43000030517578, Test Loss: 1.4557284116744995, Test Accuracy: 48.619998931884766\n",
            "Epoch 36, Loss: 1.2536708116531372, Accuracy: 55.35200119018555, Test Loss: 1.4921846389770508, Test Accuracy: 47.47999954223633\n",
            "Epoch 37, Loss: 1.248661994934082, Accuracy: 55.391998291015625, Test Loss: 1.4435352087020874, Test Accuracy: 49.20000076293945\n",
            "Epoch 38, Loss: 1.2468079328536987, Accuracy: 55.295997619628906, Test Loss: 1.431325912475586, Test Accuracy: 49.41999816894531\n",
            "Epoch 39, Loss: 1.2409394979476929, Accuracy: 55.615997314453125, Test Loss: 1.4520001411437988, Test Accuracy: 49.18000030517578\n",
            "Epoch 40, Loss: 1.2409348487854004, Accuracy: 55.66400146484375, Test Loss: 1.5143725872039795, Test Accuracy: 46.7400016784668\n",
            "Epoch 41, Loss: 1.2366607189178467, Accuracy: 55.68199920654297, Test Loss: 1.4524002075195312, Test Accuracy: 48.459999084472656\n",
            "Epoch 42, Loss: 1.232858657836914, Accuracy: 55.78799819946289, Test Loss: 1.499451994895935, Test Accuracy: 48.33000183105469\n",
            "Epoch 43, Loss: 1.2287635803222656, Accuracy: 55.926002502441406, Test Loss: 1.4574400186538696, Test Accuracy: 49.0\n",
            "Epoch 44, Loss: 1.224631428718567, Accuracy: 56.13600158691406, Test Loss: 1.4921842813491821, Test Accuracy: 48.0\n",
            "Epoch 45, Loss: 1.2227320671081543, Accuracy: 56.20199966430664, Test Loss: 1.4556169509887695, Test Accuracy: 48.91999816894531\n",
            "Epoch 46, Loss: 1.2189016342163086, Accuracy: 56.21200180053711, Test Loss: 1.4723578691482544, Test Accuracy: 49.04999923706055\n",
            "Epoch 47, Loss: 1.2186199426651, Accuracy: 56.48400115966797, Test Loss: 1.488126277923584, Test Accuracy: 47.93000030517578\n",
            "Epoch 48, Loss: 1.2162266969680786, Accuracy: 56.33399963378906, Test Loss: 1.4640871286392212, Test Accuracy: 48.8599967956543\n",
            "Epoch 49, Loss: 1.2082281112670898, Accuracy: 56.79399871826172, Test Loss: 1.4587746858596802, Test Accuracy: 49.23999786376953\n",
            "Epoch 50, Loss: 1.2057520151138306, Accuracy: 56.847999572753906, Test Loss: 1.4632760286331177, Test Accuracy: 49.04999923706055\n",
            "Epoch 51, Loss: 1.208577275276184, Accuracy: 56.8859977722168, Test Loss: 1.462268590927124, Test Accuracy: 48.87999725341797\n",
            "Epoch 52, Loss: 1.2061408758163452, Accuracy: 57.10399627685547, Test Loss: 1.4834233522415161, Test Accuracy: 49.189998626708984\n",
            "Epoch 53, Loss: 1.200545310974121, Accuracy: 57.0, Test Loss: 1.4558348655700684, Test Accuracy: 49.779998779296875\n",
            "Epoch 54, Loss: 1.2012742757797241, Accuracy: 57.040000915527344, Test Loss: 1.4906693696975708, Test Accuracy: 48.52000045776367\n",
            "Epoch 55, Loss: 1.1982094049453735, Accuracy: 57.167999267578125, Test Loss: 1.5021193027496338, Test Accuracy: 48.48999786376953\n",
            "Epoch 56, Loss: 1.1964688301086426, Accuracy: 57.205997467041016, Test Loss: 1.5123624801635742, Test Accuracy: 48.130001068115234\n",
            "Epoch 57, Loss: 1.191834807395935, Accuracy: 57.417999267578125, Test Loss: 1.4708566665649414, Test Accuracy: 49.15999984741211\n",
            "Epoch 58, Loss: 1.1855024099349976, Accuracy: 57.56599807739258, Test Loss: 1.483554482460022, Test Accuracy: 48.709999084472656\n",
            "Epoch 59, Loss: 1.1920325756072998, Accuracy: 57.073997497558594, Test Loss: 1.4720323085784912, Test Accuracy: 48.90999984741211\n",
            "Epoch 60, Loss: 1.184207797050476, Accuracy: 57.80799865722656, Test Loss: 1.4815245866775513, Test Accuracy: 48.8599967956543\n",
            "Epoch 61, Loss: 1.1823070049285889, Accuracy: 57.55799865722656, Test Loss: 1.4703004360198975, Test Accuracy: 49.52000045776367\n",
            "Epoch 62, Loss: 1.1815786361694336, Accuracy: 57.656002044677734, Test Loss: 1.4715311527252197, Test Accuracy: 48.98999786376953\n",
            "Epoch 63, Loss: 1.1786376237869263, Accuracy: 57.84000015258789, Test Loss: 1.4581483602523804, Test Accuracy: 49.73999786376953\n",
            "Epoch 64, Loss: 1.1755518913269043, Accuracy: 57.85400390625, Test Loss: 1.4579036235809326, Test Accuracy: 49.380001068115234\n",
            "Epoch 65, Loss: 1.1756747961044312, Accuracy: 57.89200210571289, Test Loss: 1.5246862173080444, Test Accuracy: 48.20000076293945\n",
            "Epoch 66, Loss: 1.1732423305511475, Accuracy: 57.96399688720703, Test Loss: 1.5255568027496338, Test Accuracy: 47.81999969482422\n",
            "Epoch 67, Loss: 1.1697797775268555, Accuracy: 58.145999908447266, Test Loss: 1.4707210063934326, Test Accuracy: 49.11000061035156\n",
            "Epoch 68, Loss: 1.1690144538879395, Accuracy: 58.23600387573242, Test Loss: 1.5090655088424683, Test Accuracy: 48.01000213623047\n",
            "Epoch 69, Loss: 1.1647228002548218, Accuracy: 58.194000244140625, Test Loss: 1.501523494720459, Test Accuracy: 48.65999984741211\n",
            "Epoch 70, Loss: 1.1621896028518677, Accuracy: 58.42000198364258, Test Loss: 1.508815884590149, Test Accuracy: 48.209999084472656\n",
            "Epoch 71, Loss: 1.1635353565216064, Accuracy: 58.29600143432617, Test Loss: 1.5068618059158325, Test Accuracy: 48.869998931884766\n",
            "Epoch 72, Loss: 1.1553741693496704, Accuracy: 58.45800018310547, Test Loss: 1.521378993988037, Test Accuracy: 47.69000244140625\n",
            "Epoch 73, Loss: 1.1649442911148071, Accuracy: 58.31999969482422, Test Loss: 1.5245845317840576, Test Accuracy: 47.560001373291016\n",
            "Epoch 74, Loss: 1.1549816131591797, Accuracy: 58.447998046875, Test Loss: 1.5104711055755615, Test Accuracy: 48.7599983215332\n",
            "Epoch 75, Loss: 1.1544232368469238, Accuracy: 58.784000396728516, Test Loss: 1.489731788635254, Test Accuracy: 49.189998626708984\n",
            "Epoch 76, Loss: 1.1519683599472046, Accuracy: 58.67599868774414, Test Loss: 1.510473370552063, Test Accuracy: 48.72999954223633\n",
            "Epoch 77, Loss: 1.1539541482925415, Accuracy: 58.72600173950195, Test Loss: 1.543358564376831, Test Accuracy: 47.68000030517578\n",
            "Epoch 78, Loss: 1.1510026454925537, Accuracy: 58.70199966430664, Test Loss: 1.514955759048462, Test Accuracy: 48.43000030517578\n",
            "Epoch 79, Loss: 1.1497159004211426, Accuracy: 58.49599838256836, Test Loss: 1.5361583232879639, Test Accuracy: 48.220001220703125\n",
            "Epoch 80, Loss: 1.1466857194900513, Accuracy: 58.9219970703125, Test Loss: 1.5038405656814575, Test Accuracy: 49.36000061035156\n",
            "Epoch 81, Loss: 1.1424996852874756, Accuracy: 59.00600051879883, Test Loss: 1.5332773923873901, Test Accuracy: 48.209999084472656\n",
            "Epoch 82, Loss: 1.141045093536377, Accuracy: 59.15399932861328, Test Loss: 1.5475482940673828, Test Accuracy: 47.459999084472656\n",
            "Epoch 83, Loss: 1.1383116245269775, Accuracy: 59.12600326538086, Test Loss: 1.5032466650009155, Test Accuracy: 48.79999923706055\n",
            "Epoch 84, Loss: 1.1376272439956665, Accuracy: 59.4640007019043, Test Loss: 1.517350673675537, Test Accuracy: 48.93000030517578\n",
            "Epoch 85, Loss: 1.134465217590332, Accuracy: 59.478004455566406, Test Loss: 1.506109356880188, Test Accuracy: 47.9900016784668\n",
            "Epoch 86, Loss: 1.1332752704620361, Accuracy: 59.32999801635742, Test Loss: 1.5024367570877075, Test Accuracy: 48.63999938964844\n",
            "Epoch 87, Loss: 1.1319270133972168, Accuracy: 59.413997650146484, Test Loss: 1.49652099609375, Test Accuracy: 48.220001220703125\n",
            "Epoch 88, Loss: 1.1350078582763672, Accuracy: 59.236000061035156, Test Loss: 1.5528438091278076, Test Accuracy: 47.220001220703125\n",
            "Epoch 89, Loss: 1.1274186372756958, Accuracy: 59.57999801635742, Test Loss: 1.564469814300537, Test Accuracy: 48.03000259399414\n",
            "Epoch 90, Loss: 1.1280876398086548, Accuracy: 59.72999954223633, Test Loss: 1.531496524810791, Test Accuracy: 47.87000274658203\n",
            "Epoch 91, Loss: 1.1277282238006592, Accuracy: 59.54399871826172, Test Loss: 1.5355409383773804, Test Accuracy: 48.31999969482422\n",
            "Epoch 92, Loss: 1.1226818561553955, Accuracy: 59.70800018310547, Test Loss: 1.5173118114471436, Test Accuracy: 49.09000015258789\n",
            "Epoch 93, Loss: 1.1260745525360107, Accuracy: 59.534000396728516, Test Loss: 1.5577468872070312, Test Accuracy: 48.10000228881836\n",
            "Epoch 94, Loss: 1.1256928443908691, Accuracy: 59.465999603271484, Test Loss: 1.5374032258987427, Test Accuracy: 47.85000228881836\n",
            "Epoch 95, Loss: 1.1250046491622925, Accuracy: 59.7760009765625, Test Loss: 1.5289359092712402, Test Accuracy: 48.57999801635742\n",
            "Epoch 96, Loss: 1.1192575693130493, Accuracy: 60.00199890136719, Test Loss: 1.5203877687454224, Test Accuracy: 49.029998779296875\n",
            "Epoch 97, Loss: 1.1185871362686157, Accuracy: 60.07400131225586, Test Loss: 1.5394821166992188, Test Accuracy: 47.62000274658203\n",
            "Epoch 98, Loss: 1.1142539978027344, Accuracy: 60.07999801635742, Test Loss: 1.5456961393356323, Test Accuracy: 48.52000045776367\n",
            "Epoch 99, Loss: 1.1137956380844116, Accuracy: 60.022003173828125, Test Loss: 1.5376527309417725, Test Accuracy: 48.45000076293945\n",
            "Epoch 100, Loss: 1.112892508506775, Accuracy: 60.0780029296875, Test Loss: 1.5333420038223267, Test Accuracy: 48.689998626708984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFE_Ttv0plJ-",
        "colab_type": "code",
        "outputId": "c443414e-58f7-421f-8a4b-a23107c3b545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!tensorboard dev upload --logdir \"$log_dir\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Upload started and will continue reading any new data as it's added\n",
            "to the logdir. To stop uploading, press Ctrl-C.\n",
            "View your TensorBoard live at: https://tensorboard.dev/experiment/5TvWKdaFQy66M9P6ipN6kQ/\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 605, in execute\n",
            "    uploader.start_uploading()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader.py\", line 165, in start_uploading\n",
            "    self._logdir_poll_rate_limiter.tick()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/util.py\", line 45, in tick\n",
            "    self._time.sleep(wait_secs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 66, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 268, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 788, in run\n",
            "    return _run(flags)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 323, in _run\n",
            "    intent.execute(server_info, channel)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_main.py\", line 610, in execute\n",
            "    print()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gp3ZsXRjdSb",
        "colab_type": "text"
      },
      "source": [
        "## 1e) Include a brief written answer to the following questions:\n",
        "- Did Swish help your NNs reach a higher validation accuracy? \n",
        "- Did it reduce the time (in terms of training epochs) needed to reach a certain accuracy?\n",
        "- Please include the URLs showing the results of your experiments in TensorBoard.dev to justify your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCxpZ5PkwP6",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Results\n",
        "https://tensorboard.dev/experiment/5TvWKdaFQy66M9P6ipN6kQ/\n",
        "\n",
        "# Analysis\n",
        "\n",
        "Although in general the 2 networks preformed relatively similarely, it can be seen that the network utelizing the swish activation functions achieved a slightly higher validation accuracy (about 49%) than the network utelizing relu activation functions (about 48%).\n",
        "\n",
        "As for the time required to acieve these validation accuracy there is no noticable difference when the training noise is taken into account"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3h8P2lylHFF",
        "colab_type": "text"
      },
      "source": [
        "# Section 2: Use LIME to explain an image classifier\n",
        "\n",
        "Explaining and interpreting models is a new and increasingly important area of Deep Learning. In this section, you will gain experience using a recent (and relatively simple) technique called LIME.\n",
        "\n",
        "## Written answers\n",
        "\n",
        "## 2a) Read the paper [ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases](https://arxiv.org/abs/1711.11443) then answer the following two questions:\n",
        "- Why would a model misclassify a professional sports player based on the color of their skin?\n",
        "- What changes would you make to the training set to correct this behavior?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-uVNVbvjE_Y",
        "colab_type": "text"
      },
      "source": [
        "### Why would a model misclassify a professional sports player based on the color of their skin?\n",
        "\n",
        "One hypothesis is that despite the balanced distribution of races in pictures labeled basketball, black persons are more represented in this class in comparison to the other classes.\n",
        "\n",
        "### What changes would you make to the training set to correct this behavior?\n",
        "\n",
        "Ensure that there is a balanced distribution of races in ALL of the individual subest of examples corresponding to ALL of the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr1FiS20k57c",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Programming questions\n",
        "\n",
        "## 2b) Read the paper [\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) then practice using LIME by adding code below to do the following:\n",
        "- Install LIME\n",
        "- Download an image from the web (using `!wget` or a similar utility).\n",
        "- Classify your image using Inception-V3 (or another famous architecture, using weights pretrained on ImageNet).\n",
        "- Display the top three predicted classes (e.g., baseball player) and confidence scores.\n",
        "- Use LIME to provide evidence for and against each of the top three predictions (e.g., display the regions of an image that LIME found to correlate most strongly with and against the predicted class).\n",
        "\n",
        "Save your output inside this notebook and include it with your submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZVX3z9ak9G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# You may add additional text cells if helpful\n",
        "!pip install lime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXbHV3Gdk7i8",
        "colab_type": "text"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "If you like to learn about another promising approach to explaining NNs, you can read the paper [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6ZYCN2eoC7",
        "colab_type": "text"
      },
      "source": [
        "# Section 3: Use Keras Tuner to optimize a small model\n",
        "\n",
        "\n",
        "## Written answers\n",
        "\n",
        "## 3a) When and why might Grid Search be less effective than Random Search when searching for useful hyperparameters for a Deep Learning model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKQXRhCblS-J",
        "colab_type": "text"
      },
      "source": [
        "### TODO: your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2km45q83lVcV",
        "colab_type": "text"
      },
      "source": [
        "## Programming questions\n",
        "\n",
        "**3b) Add one or more code cells below in which you complete the following:**\n",
        "- Install Keras Tuner\n",
        "- Write a CNN to classify images from CIFAR-10 \n",
        "- Use Keras Tuner to search for at least three optimal hyperpameters for your model (eg, number of layers, number of filters per layer, dropout rate, etc)\n",
        "\n",
        "Save your output in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cth4UWZelU9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here\n",
        "# You may add additional code cells if helpful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBVypFcola6l",
        "colab_type": "text"
      },
      "source": [
        "## 3c) In the text cell below, brielfy answer the following questions:\n",
        "- What were the optimal hyperparmeters you found?\n",
        "- Do they make sense (e.g., are they similar to parameters you might have picked yourself?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYYvxfpOldSt",
        "colab_type": "text"
      },
      "source": [
        "### TODO: your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjU1L133f4sp",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSc6v5ws8wKS",
        "colab_type": "text"
      },
      "source": [
        "## Starter code for TensorBoard\n",
        "The following code shows how to use [TensorBoard](https://www.tensorflow.org/tensorboard) to display the results from an experiment comparing two learning curves. Please note, there are three ways to use TensorBoard. \n",
        "- You may install TensorBoard locally on your laptop\n",
        "- You can run TensorBoard inside Colab (currently buggy)\n",
        "- You can use TensorBoard.dev (this is the approach you should use for this assignment). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE5lIlzkEUAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "599JJ57yl1rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpe_UINKKLDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehUDr72LLAJY"
      },
      "source": [
        "**Caution**. The following cell will delete any existing TensorBoard logs. If you're running this on your local machine, please be careful executing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScjIcVAJKkmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ6aWI_NLm6C",
        "colab_type": "text"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByT9RfkIK4kD",
        "colab_type": "code",
        "outputId": "22ffcb3a-d584-4651-a8b0-7ca63c31abbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2_huYeq80tv",
        "colab_type": "text"
      },
      "source": [
        "## First style\n",
        "The following code shows how to use TensorBoard with ```model.fit```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHBo5cgTLuis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEDGbBRMI-_",
        "colab_type": "text"
      },
      "source": [
        "Create a logs directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggu_lZRnL6Uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime \n",
        "import os\n",
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_icHggivMBXe",
        "colab_type": "text"
      },
      "source": [
        "### Run an experiment\n",
        "The name of the experiment is given by the path of the logs directory (here, \"exp1\"). You'll want to use something more descriptive in your work (e.g., \"swish-cifar-10\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyVj4vmLM0Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.0, nesterov=False) \n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"exp1\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6-cL05DN9fR",
        "colab_type": "text"
      },
      "source": [
        "### Run a second experiment\n",
        "Let's train another model, this time saving results to \"exp2\". Later, we'll be able to compare the learning curves of these experiments in TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XVPmrpeN_u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"exp2\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_pSh18ZOENe",
        "colab_type": "text"
      },
      "source": [
        "### Upload the logs to TensorBoard.dev, and compare the results\n",
        "TensorBoard.dev is a hosted version of TensorBoard (see http://tensorboard.dev/ for details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NwwW6OEUAIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard dev upload --logdir \"$log_dir\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPLDEq8MSQYH",
        "colab_type": "text"
      },
      "source": [
        "## Second style\n",
        "Showing how to use TensorBoard with a Subclassed model and a GradientTape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vV0Hz2RV1C5",
        "colab_type": "text"
      },
      "source": [
        "Prepre the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX0umywbPeW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_ds = train_ds.shuffle(60000).batch(32)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZgdwTLLwq1D",
        "colab_type": "text"
      },
      "source": [
        "Define a simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRaR1rtpUgDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    return self.d1(x)\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMFRp7CCTxmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aIkldyT1S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZzldV1BV3Rs",
        "colab_type": "text"
      },
      "source": [
        "Training and testing routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDHIccXUP_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEUmM7G6V7ae",
        "colab_type": "text"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGrEExdWV84_",
        "colab_type": "code",
        "outputId": "fde13255-6f4a-4bce-abf0-274b72f61570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"test\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20200321-152917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v-gMTCiwuBO",
        "colab_type": "text"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rxTf4CyVKxK",
        "colab_type": "code",
        "outputId": "cc83bb27-8485-4eb9-cec9-854046c25a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4738991856575012, Accuracy: 87.65833282470703, Test Loss: 0.30589091777801514, Test Accuracy: 91.52999877929688\n",
            "Epoch 2, Loss: 0.30400350689888, Accuracy: 91.5816650390625, Test Loss: 0.28006136417388916, Test Accuracy: 92.25\n",
            "Epoch 3, Loss: 0.2830759882926941, Accuracy: 92.10166931152344, Test Loss: 0.27733173966407776, Test Accuracy: 92.29999542236328\n",
            "Epoch 4, Loss: 0.2731097638607025, Accuracy: 92.36000061035156, Test Loss: 0.27061912417411804, Test Accuracy: 92.44999694824219\n",
            "Epoch 5, Loss: 0.2666522264480591, Accuracy: 92.5633316040039, Test Loss: 0.2657223045825958, Test Accuracy: 92.69999694824219\n",
            "Epoch 6, Loss: 0.26179182529449463, Accuracy: 92.6933364868164, Test Loss: 0.26932454109191895, Test Accuracy: 92.45999908447266\n",
            "Epoch 7, Loss: 0.2584364116191864, Accuracy: 92.84666442871094, Test Loss: 0.26669907569885254, Test Accuracy: 92.69000244140625\n",
            "Epoch 8, Loss: 0.2554880678653717, Accuracy: 92.86833190917969, Test Loss: 0.2611234486103058, Test Accuracy: 92.83999633789062\n",
            "Epoch 9, Loss: 0.25281357765197754, Accuracy: 93.00666809082031, Test Loss: 0.27040645480155945, Test Accuracy: 92.38999938964844\n",
            "Epoch 10, Loss: 0.25119897723197937, Accuracy: 93.038330078125, Test Loss: 0.26565492153167725, Test Accuracy: 92.58000183105469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9-eBZGIo5NZ",
        "colab_type": "text"
      },
      "source": [
        "### Upload the logs to TensorBoard.dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrXOlQa5VWnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard dev upload --logdir \"$log_dir\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}